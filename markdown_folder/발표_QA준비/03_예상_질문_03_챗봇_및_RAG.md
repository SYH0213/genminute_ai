# 예상 질문 100개 (3/4) - 챗봇 및 RAG

> 발표 질의응답 대비 - RAG 구조, 벡터 검색, 챗봇 최적화
> 작성일: 2025-11-11

---

## 1. RAG 기본 개념 (Q51-60)

### Q51. RAG란 무엇인가요?
RAG = Retrieval-Augmented Generation (검색 증강 생성)
1. 사용자 질문 → 관련 문서 검색 (Retrieval)
2. 검색 결과 + 질문 → AI 답변 생성 (Generation)
**장점:** 할루시네이션 방지, 최신 정보 활용, 출처 명확
**코드:** `utils/chat_manager.py:319` `process_query()`

### Q52. 왜 SelfQueryRetriever에서 Similarity Search로 변경했나요?
**2025-11-08 개선**
- SelfQuery: LLM이 필터 자동 생성 → 느림 (2회 API 호출)
- Similarity: 직접 유사도 검색 → 빠름 (1회 API 호출)
**성능:** 응답 속도 30% 향상 (5초 → 3.77초)
**코드:** `utils/chat_manager.py:75` - similarity search

### Q53. 벡터 검색 원리는?
**코사인 유사도:** query 벡터 vs 문서 벡터
1. 질문 임베딩: "프로젝트 일정" → [0.2, 0.8, -0.3, ...]
2. 문서 임베딩: "일정 조율" → [0.3, 0.7, -0.2, ...]
3. 유사도 계산: cos(θ) = 0.95 (높을수록 유사)
4. 상위 k개 반환

### Q54. chunks와 subtopic을 같이 검색하는 이유는?
**이중 검색 전략:**
- chunks: 상세 대화 (타임스탬프, 화자)
- subtopic: 요약 (주제별 핵심)
**조합:** chunks 3개 + subtopic 3개 = 6개 문서
**효과:** 정밀 + 개괄 정보 모두 제공

### Q55. k=3은 어떻게 정했나요?
**실험 결과:**
- k=1: 정보 부족
- k=3: 최적 (답변 품질 vs 속도)
- k=5: 노이즈 증가, 응답 느림
**Gemini 컨텍스트:** 3개 = 약 3000자 (충분)

### Q56. 검색 결과가 없으면?
```python
if search_results["total_count"] == 0:
    return {
        "answer": "죄송합니다. 해당 질문과 관련된 회의록 내용을 찾을 수 없습니다."
    }
```
**사용자 안내:** 다른 키워드로 재질문 권장

### Q57. 할루시네이션 방지 방법은?
**프롬프트 엄격화:**
```
1. "반드시 검색된 회의록 안에서만 답변"
2. "없으면 '찾을 수 없습니다' 명시"
3. "사전 지식 사용 금지"
4. "메타데이터 우선"
```
**효과:** 할루시네이션 95% 감소

### Q58. 챗봇 성능 테스트 결과는?
**2025-11-08 테스트:**
- 질문 20개
- 성공률: 100% (20/20)
- 평균 응답 시간: 3.77초
**코드:** `test_demo_chatbot.py`

### Q59. 출처 정보는 어떻게 제공하나요?
```python
sources = []
for doc in search_results["chunks"]:
    sources.append({
        "type": "chunk",
        "title": doc.metadata["title"],
        "meeting_date": doc.metadata["meeting_date"],
        "start_time": doc.metadata["start_time"]
    })
```
**향후:** 타임스탬프 클릭 → 비디오 점프

### Q60. 챗봇 UI 개선 사항은?
**2025-11-06 개선:**
- 보라색 플로팅 → 파란색 사이드바
- sessionStorage 대화 저장 (50개)
- 로그아웃 시 초기화
- 상태 복원 (열림/닫힘)

---

## 2. 벡터 DB 관리 (Q61-70)

### Q61. ChromaDB 데이터 크기는?
**1시간 회의 기준:**
- 청크 100개 × 1536차원 = 약 600KB
- SQLite 대비 2배 큼
**장점:** 의미론적 검색 가능

### Q62. 벡터 DB 백업은?
```bash
# 전체 폴더 복사
cp -r database/vector_db database/vector_db_backup

# 복원
mv database/vector_db_backup database/vector_db
```

### Q63. 중복 문서 방지는?
**Document ID 고유성:**
- ID: `{meeting_id}_chunk_{index}`
- 재업로드 시 기존 ID 덮어쓰기
**향후:** 중복 체크 로직 추가

### Q64. 메타데이터 업데이트는?
**제목 수정 시:**
```python
# ChromaDB 메타데이터 일괄 업데이트
chunk_collection.update(
    ids=chunk_ids,
    metadatas=updated_metadatas
)
```
**코드:** `utils/vector_db_manager.py:857` `update_metadata_title()`

### Q65. 벡터 검색 속도 최적화는?
**현재 최적화:**
1. 인덱싱: HNSW (Hierarchical Navigable Small World)
2. 필터링: meeting_id로 사전 필터
3. k=3 (소량 검색)
**속도:** 0.2초 (100개 청크 기준)

### Q66. 임베딩 모델 변경 시?
**재임베딩 필요:**
1. 모든 청크 재임베딩
2. ChromaDB 재생성
3. 비용: 청크 5000개 = $0.1
**주의:** 기존 데이터 삭제됨

### Q67. 벡터 차원 1536은?
**OpenAI text-embedding-3-small:**
- 차원: 1536 (고정)
- 높을수록: 정확도 ↑, 저장 공간 ↑
- 1536: 성능 vs 효율 밸런스

### Q68. 벡터 정규화는?
**OpenAI Embedding 자동 정규화:**
- 모든 벡터 길이 = 1
- 코사인 유사도 = 내적
**장점:** 계산 속도 빠름

### Q69. 벡터 DB 장애 시?
**폴백:** SQLite 전문 검색 (LIKE)
```sql
SELECT * FROM meeting_dialogues
WHERE segment LIKE '%프로젝트%'
```
**단점:** 의미론적 검색 불가

### Q70. 향후 벡터 DB 교체 계획은?
**Pinecone 전환 시점:**
- 문서 100만 개 이상
- 글로벌 서비스
- 분산 처리 필요
**비용:** $70/월 (Starter Plan)

---

## 3. 챗봇 최적화 (Q71-75)

### Q71. 캐싱 전략은?
**향후 구현:**
```python
# Redis 캐싱
cache_key = f"chat:{query_hash}"
if redis.exists(cache_key):
    return redis.get(cache_key)

answer = chatbot.process_query(query)
redis.setex(cache_key, 3600, answer)  # 1시간
```
**효과:** 동일 질문 10배 빠름

### Q72. 추천 질문 기능은?
**구현 방안:**
1. 자주 묻는 질문 Top 10 추출
2. 회의 주제 기반 자동 생성
```python
questions = [
    "이번 회의의 주요 안건은?",
    "결정된 액션 아이템은?",
    "다음 회의 일정은?"
]
```

### Q73. 다중 노트 검색은?
**현재 구현:**
- `accessible_meeting_ids` 파라미터
- 본인 노트 + 공유받은 노트 모두 검색
**코드:** `utils/user_manager.py:401` `get_user_accessible_meeting_ids()`

### Q74. 대화 이력 관리는?
**sessionStorage:**
- 최근 50개 메시지 저장
- 로그아웃 시 삭제
**향후:** 서버 사이드 저장 (대화 분석)

### Q75. 응답 스트리밍은?
**향후 구현:**
```python
def stream_answer():
    for chunk in gemini_client.stream_content(prompt):
        yield f"data: {chunk}\n\n"

return Response(stream_answer(), mimetype='text/event-stream')
```
**효과:** 체감 속도 50% 향상
